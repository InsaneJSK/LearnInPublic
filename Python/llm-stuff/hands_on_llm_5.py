# -*- coding: utf-8 -*-
"""Hands-on-llm

Automatically generated by Colab.
"""

!pip install bertopic --q

from datasets import load_dataset

data = load_dataset("maartengr/arxiv_nlp")["train"]

abstracts = list(data["Abstracts"])
titles = list(data["Titles"])

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("thenlper/gte-small")
embeddings = embedding_model.encode(abstracts, show_progress_bar=True)

embeddings.shape

from umap import UMAP

umap_model = UMAP(
    n_components = 5,
    min_dist = 0.0,
    metric = 'cosine',
    random_state = 42
)
reduced_embeddings = umap_model.fit_transform(embeddings)

from hdbscan import HDBSCAN

hdbscan_model = HDBSCAN(
    min_cluster_size = 50,
    metric = 'euclidean',
    cluster_selection_method = 'eom',
    prediction_data = True
).fit(reduced_embeddings)

clusters = hdbscan_model.labels_

len(set(clusters))

import numpy as np
cluster = 0
for idx in np.where(clusters == cluster)[0][:3]:
  print(abstracts[idx][:300]+"...\n")

import pandas as pd

reduced_embeddings2 = UMAP(
    n_components = 2,
    min_dist = 0.0,
    metric = 'cosine',
    random_state = 42
).fit_transform(embeddings)

df = pd.DataFrame(reduced_embeddings2, columns = ['x', 'y'])
df['title'] = titles
df["cluster"] = [str(c) for c in clusters]
to_plot = df.loc[df.cluster != "-1", :]
outliers = df.loc[df.cluster == "-1", :]

import matplotlib.pyplot as plt
plt.scatter(outliers.x, outliers.y, alpha = 0.05, s = 2, c = "grey")
plt.scatter(to_plot.x, to_plot.y, c = to_plot.cluster.astype(int), alpha = 0.06, s = 2, cmap = "tab20b")
plt.axis("off")

from bertopic import BERTopic
topic_model = BERTopic(
    embedding_model = embedding_model,
    umap_model = umap_model,
    hdbscan_model = hdbscan_model,
    verbose = True
).fit(abstracts, embeddings)

topic_model.get_topic_info()

topic_model.get_topic(0)

topic_model.find_topics("Topic Modeling")

topic_model.get_topic(24)

topic_model.topics_[titles.index("BERTopic: Neural topic modeling with a class-based TF-IDF procedure")]

fig = topic_model.visualize_documents(
    titles,
    reduced_embeddings=reduced_embeddings2,
    width = 1200,
    hide_annotations = True
)

fig.update_layout(font=dict(size=16))

topic_model.visualize_barchart()

topic_model.visualize_heatmap(n_clusters = 30)

topic_model.visualize_hierarchy()

from copy import deepcopy
original_topics = deepcopy(topic_model.topic_representations_)

def topic_differences(model, original_topics, nr_topics = 5):
  df = pd.DataFrame(columns = ["Topic", "Original", "Updated"])
  for topic in range(nr_topics):
    og_words = " | ".join(list(zip(*original_topics[topic]))[0][:5])
    new_words = " | ".join(list(zip(*model.get_topic(topic)))[0][:5])
    df.loc[len(df)] = [topic, og_words, new_words]

  return df

from bertopic.representation import KeyBERTInspired

representation_model = KeyBERTInspired()
topic_model.update_topics(abstracts, representation_model=representation_model)

topic_differences(topic_model, original_topics)

from bertopic.representation import MaximalMarginalRelevance

representation_model = MaximalMarginalRelevance(diversity=0.2)
topic_model.update_topics(abstracts, representation_model=representation_model)

topic_differences(topic_model, original_topics)

from transformers import pipeline
from bertopic.representation import TextGeneration
prompt = """I have a topic that contains the following documents:
[DOCUMENTS]

The topic is described by the following keywords: '[KEYWORDS]'

Based on the documents and keywords, what is this topic about?
"""

generator = pipeline("text2text-generation", model="google/flan-t5-small")
representation_model = TextGeneration(
    generator, prompt=prompt, doc_length = 50, tokenizer = "whitespace"
)
topic_model.update_topics(abstracts, representation_model=representation_model)

topic_differences(topic_model, original_topics)

